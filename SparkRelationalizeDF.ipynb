{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkRelationalizeDF.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "iFmrQs50V5Ef",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "PySpark code to flatten any complex nested JSON/CSV/DataFrames/SQLSchema.\n",
        "\n",
        "For example, lets look at nested JSONs -\n",
        "\n",
        "*   Flattens all nested items:\n",
        "{\n",
        "\"human\":{\n",
        "    \"name\":{\n",
        "        \"first_name\":\"Jay Lohokare\"\n",
        "      }\n",
        "   }\n",
        "}\n",
        "\n",
        "Is converted to dataFrame with column = 'human-name-first_name'\n",
        "The connector '-' can be changed by changing the connector variable.\n",
        "\n",
        "\n",
        "*   Explodes Arrays:\n",
        "{\n",
        "\"array\":[\"one\", \"two\", \"three\"]\n",
        "}\n",
        "Is converted to dataFrame with column = 'array' with 3 rows\n",
        "<br><br>\n",
        "\n",
        "The function can handle any level of nesting.\n",
        "\n",
        "The function can <b>NOT</b> handle Arrays within Arrays. \n",
        "This is just to keep the code dynamic and generic. To handle Arrays within Arrays, modify ```\n",
        "if isinstance``` in the ```for``` loop of ```flattenSchema``` function\n"
      ]
    },
    {
      "metadata": {
        "id": "AVfpKZB8VsTv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#All imports go here\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StructType, ArrayType  \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cpYetsi3Y_tI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Read data\n",
        "#This example reads data from JSONs\n",
        "#However, dataframe can be loaded from any input format\n",
        "dataFrame = sqlContext.read.json(\"PATH-TO-JSONs\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Sh73NSnV18d",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def relationalize(df):\n",
        "    connector = '-'\n",
        "    def flattenSchema(schema, prefix=None):\n",
        "        fields = []\n",
        "        \n",
        "        for field in schema.fields:\n",
        "          \n",
        "            #The field.name is added with `` to handle the case where prohibited characters like '.' are a part of the JSON key\n",
        "            if '.' in field.name:\n",
        "                name = prefix + '.' + '`' + field.name + '`' if prefix else '`' + field.name + '`'\n",
        "            else:\n",
        "                name = prefix + '.' + field.name if prefix else field.name\n",
        "            dtype = field.dataType\n",
        "            \n",
        "            if isinstance(dtype, ArrayType):\n",
        "                dtype = dtype.elementType\n",
        "                \n",
        "    \n",
        "            if isinstance(dtype, StructType):\n",
        "                fields += flattenSchema(dtype, prefix=name)\n",
        "                \n",
        "            else:\n",
        "                fields.append(name)\n",
        "        return fields\n",
        "    \n",
        "    newDf = df\n",
        "    \n",
        "    for col_name in flattenSchema(df.schema):\n",
        "        newDf = newDf.withColumn(col_name.replace('`', '').replace('.', connector), col(col_name))\n",
        "    \n",
        "    for field in newDf.schema:\n",
        "        if isinstance(field.dataType, StructType):\n",
        "            newDf = newDf.drop(field.name)\n",
        "        elif isinstance(field.dataType, ArrayType):\n",
        "            from pyspark.sql.functions import explode_outer\n",
        "            newDf = newDf.withColumn(field.name, explode_outer(field.name))\n",
        "            \n",
        "        else:\n",
        "            continue\n",
        "            \n",
        "    newDf.show()\n",
        "    return newDf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KoPwJfY4Z6Ps",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Get the flattened dataframe\n",
        "flattenedDf = relationalize(dataFrame)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bCXZoH8UV15U",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Writing the data back\n",
        "#This is example for writing JSON\n",
        "#However, Dataframes can be stored into many other data formats\n",
        "flattenedDf.write.json(\"PATH-TO-WRITE-JSONs\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}